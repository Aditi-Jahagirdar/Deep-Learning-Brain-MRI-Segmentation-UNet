{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install utils\n",
    "!pip install unet\n",
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow\n",
    "#print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "import h5py\n",
    "import tqdm\n",
    "\n",
    "import cv2\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import skimage.morphology\n",
    "import skimage.color\n",
    "import sklearn.model_selection\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from glob import glob\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from skimage.color import rgb2gray\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.models import Model, load_model, save_model\n",
    "from tensorflow.keras.layers import Input, Activation, BatchNormalization, Dropout, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# from utils import *\n",
    "# from unet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Size parameters of image\n",
    "im_width = 256\n",
    "im_height = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create two lists for mask image and tif image\n",
    "#1. list of all files containing word 'mask' \n",
    "image_filenames_train = []\n",
    "mask_files = glob('/kaggle/input/lgg-mri-segmentation/kaggle_3m/*/*_mask*')\n",
    "\n",
    "#list of train files not containing 'mask'\n",
    "for i in mask_files:\n",
    "  image_filenames_train.append(i.replace('_mask', ''))\n",
    "\n",
    "print(image_filenames_train[:10])\n",
    "len(image_filenames_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_from_img_path(rows,columns, list_img_path, list_mask_path):\n",
    "    fig = plt.figure(figsize = (12,12))\n",
    "    for i in range(1,rows * columns + 1):\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        img_path = list_img_path[i]\n",
    "        mask_path = list_mask_path[i]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_path)\n",
    "        plt.imshow(image)\n",
    "        plt.imshow(mask, alpha =0.4)\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_from_img_path(3, 3, image_filenames_train, mask_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Data frame & Split data on Train set, Validation set , Test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'image_filenames_train':image_filenames_train,'mask':mask_files})\n",
    "#split training and test data. 10% for test data\n",
    "df_train, df_test = train_test_split(df, test_size=0.1)\n",
    "\n",
    "#split training and validation data. 20% for validation data\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.2)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "print(df_val.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Generator, Data Augmentation and adjust data**\n",
    "Data Generator - prepare dataset to pass data in batches to the model.\n",
    "Data Augmentation  - applying transformations to data.(Increase size & quality of dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Referencing code from- https://github.com/zhixuhao/unet/blob/master/data.py\n",
    "def train_generator(\n",
    "    data_frame,\n",
    "    batch_size,\n",
    "    augmentation_dict,\n",
    "    image_color_mode=\"rgb\",\n",
    "    mask_color_mode=\"grayscale\",\n",
    "    image_save_prefix=\"image\",\n",
    "    mask_save_prefix=\"mask\",\n",
    "    save_to_dir=None,\n",
    "    target_size=(256, 256),\n",
    "    seed=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    can generate image and mask at the same time use the same seed for\n",
    "    image_datagen and mask_datagen to ensure the transformation for image\n",
    "    and mask is the same if you want to visualize the results of generator,\n",
    "    set save_to_dir = \"your path\"\n",
    "    \"\"\"\n",
    "    image_datagen = ImageDataGenerator(**augmentation_dict)\n",
    "    mask_datagen = ImageDataGenerator(**augmentation_dict)\n",
    "\n",
    "    image_generator = image_datagen.flow_from_dataframe(\n",
    "        data_frame,\n",
    "        x_col=\"image_filenames_train\",\n",
    "        class_mode=None,\n",
    "        color_mode=image_color_mode,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        save_to_dir=save_to_dir,\n",
    "        save_prefix=image_save_prefix,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    mask_generator = mask_datagen.flow_from_dataframe(\n",
    "        data_frame,\n",
    "        x_col=\"mask\",\n",
    "        class_mode=None,\n",
    "        color_mode=mask_color_mode,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        save_to_dir=save_to_dir,\n",
    "        save_prefix=mask_save_prefix,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    train_gen = zip(image_generator, mask_generator)\n",
    "    \n",
    "    # Final return Tuple after image Normalization and Diagnostics\n",
    "    for (img, mask) in train_gen:\n",
    "        img, mask = normalize_and_diagnose(img, mask)\n",
    "        yield (img, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After normalization or adjusting data , if value is <=0.5 then mask \n",
    "#is complete black and it does not have tumor\n",
    "def normalize_and_diagnose(img, mask):\n",
    "    img = img/255\n",
    "    mask = mask/255\n",
    "    mask[mask>0.5] = 1\n",
    "    mask[mask<=0.5] = 0\n",
    "    return (img,mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utility Function for dice loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficients(y_true, y_pred, smooth=100):\n",
    "    y_true_flatten = K.flatten(y_true)\n",
    "    y_pred_flatten = K.flatten(y_pred) \n",
    "    intersection = K.sum(y_true_flatten * y_pred_flatten)\n",
    "    union = K.sum(y_true_flatten) + K.sum(y_pred_flatten)\n",
    "    return (2*intersection+smooth)/ (union+smooth)\n",
    "\n",
    "def dice_coefficient_loss(y_true, y_pred, smooth=100):\n",
    "    return -dice_coefficients(y_true, y_pred, smooth)\n",
    "\n",
    "#iou - intersection over union\n",
    "def iou(y_true, y_pred, smooth=100):\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    sum = K.sum(y_true + y_pred)\n",
    "    iou = (intersection+smooth)/(sum-intersection+smooth)\n",
    "    return iou\n",
    "\n",
    "def jaccard_distance(y_true, y_pred):\n",
    "    y_true_flatten = K.flatten(y_true)\n",
    "    y_pred_flatten = K.flatten(y_pred)\n",
    "    return -iou(y_true_flatten, y_pred_flatten)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UNet Model Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(input_size=(256, 256, 3)):\n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    #Encoding leg\n",
    "    conv1 = Conv2D(filters=64, kernel_size =(3,3), padding=\"same\")(inputs)\n",
    "    batchNorm1 = Activation(\"relu\")(conv1)\n",
    "    conv1 = Conv2D(filters=64, kernel_size =(3,3), padding=\"same\")(batchNorm1)\n",
    "    batchNorm1 = BatchNormalization(axis=3)(conv1)\n",
    "    batchNorm1 = Activation(\"relu\")(batchNorm1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2,2))(batchNorm1)\n",
    "    \n",
    "    conv2 = Conv2D(filters=128, kernel_size =(3,3), padding=\"same\")(pool1)\n",
    "    batchNorm2 = Activation(\"relu\")(conv2)\n",
    "    conv2 = Conv2D(filters=128, kernel_size =(3,3), padding=\"same\")(batchNorm2)\n",
    "    batchNorm2 = BatchNormalization(axis=3)(conv2)\n",
    "    batchNorm2 = Activation(\"relu\")(batchNorm2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2,2))(batchNorm2)\n",
    "    \n",
    "    conv3 = Conv2D(filters=256, kernel_size =(3,3), padding=\"same\")(pool2)\n",
    "    batchNorm3 = Activation(\"relu\")(conv3)\n",
    "    conv3 = Conv2D(filters=256, kernel_size =(3,3), padding=\"same\")(batchNorm3)\n",
    "    batchNorm3 = BatchNormalization(axis=3)(conv3)\n",
    "    batchNorm3 = Activation(\"relu\")(batchNorm3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2,2))(batchNorm3)\n",
    "    \n",
    "    conv4 = Conv2D(filters=512, kernel_size =(3,3), padding=\"same\")(pool3)\n",
    "    batchNorm4 = Activation(\"relu\")(conv4)\n",
    "    conv4 = Conv2D(filters=512, kernel_size =(3,3), padding=\"same\")(batchNorm4)\n",
    "    batchNorm4 = BatchNormalization(axis=3)(conv4)\n",
    "    batchNorm4 = Activation(\"relu\")(batchNorm4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2,2))(batchNorm4)\n",
    "    \n",
    "    conv5 = Conv2D(filters=1024, kernel_size =(3,3), padding=\"same\")(pool4)\n",
    "    batchNorm5 = Activation(\"relu\")(conv5)\n",
    "    conv5 = Conv2D(filters=1024, kernel_size =(3,3), padding=\"same\")(batchNorm5)\n",
    "    batchNorm5 = BatchNormalization(axis=3)(conv5)\n",
    "    batchNorm5 = Activation(\"relu\")(batchNorm5)\n",
    "    \n",
    "    #Upconvolution/Decoder Leg - starts Conv2DTranspose\n",
    "    #u6=u6+c4\n",
    "    up6 = concatenate([Conv2DTranspose(512,kernel_size=(2,2),strides=(2,2),padding=\"same\")(batchNorm5),conv4,],axis=3,)\n",
    "    conv6 = Conv2D(filters=512, kernel_size=(3,3),padding=\"same\")(up6)\n",
    "    batchNorm6 = Activation(\"relu\")(conv6)\n",
    "    conv6 = Conv2D(filters=512, kernel_size=(3,3),padding=\"same\")(batchNorm6)\n",
    "    batchNorm6 = BatchNormalization(axis=3)(conv6)\n",
    "    batchNorm6 = Activation(\"relu\")(batchNorm6)\n",
    "    \n",
    "     #u7=u7+c3\n",
    "    up7 = concatenate([Conv2DTranspose(256,kernel_size=(2,2),strides=(2,2),padding=\"same\")(batchNorm6),conv3,],axis=3,)\n",
    "    conv7 = Conv2D(filters=256, kernel_size=(3,3),padding=\"same\")(up7)\n",
    "    batchNorm7 = Activation(\"relu\")(conv7)\n",
    "    conv7 = Conv2D(filters=256, kernel_size=(3,3),padding=\"same\")(batchNorm7)\n",
    "    batchNorm7 = BatchNormalization(axis=3)(conv7)\n",
    "    batchNorm7 = Activation(\"relu\")(batchNorm7)\n",
    "    \n",
    "    #u8=u8+c2\n",
    "    up8 = concatenate([Conv2DTranspose(128,kernel_size=(2,2),strides=(2,2),padding=\"same\")(batchNorm7),conv2,],axis=3,)\n",
    "    conv8 = Conv2D(filters=128, kernel_size=(3,3),padding=\"same\")(up8)\n",
    "    batchNorm8 = Activation(\"relu\")(conv8)\n",
    "    conv8 = Conv2D(filters=128, kernel_size=(3,3),padding=\"same\")(batchNorm8)\n",
    "    batchNorm8 = BatchNormalization(axis=3)(conv8)\n",
    "    batchNorm8 = Activation(\"relu\")(batchNorm8)\n",
    "    \n",
    "    #u9=u9+c1\n",
    "    up9 = concatenate([Conv2DTranspose(64,kernel_size=(2,2),strides=(2,2),padding=\"same\")(batchNorm8),conv1,],axis=3,)\n",
    "    conv9 = Conv2D(filters=64, kernel_size=(3,3),padding=\"same\")(up9)\n",
    "    batchNorm9 = Activation(\"relu\")(conv9)\n",
    "    conv9 = Conv2D(filters=64, kernel_size=(3,3),padding=\"same\")(batchNorm9)\n",
    "    batchNorm9 = BatchNormalization(axis=3)(conv9)\n",
    "    batchNorm9 = Activation(\"relu\")(batchNorm9)\n",
    "    \n",
    "    conv10 = Conv2D(filters=1, kernel_size=(1,1),activation =\"sigmoid\")(batchNorm9)\n",
    "    \n",
    "#     if(pretrained_weights):\n",
    "#     \tmodel.load_weights(pretrained_weights)\n",
    "        \n",
    "    return Model(inputs=[inputs],outputs=[conv10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hyperparameters\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "smooth =100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main- Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet(input_size=(im_height, im_width, 3))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_param = dict(rotation_range= 0.2, width_shift_range=0.05,height_shift_range=0.05,shear_range=0.05, zoom_range=0.05, horizontal_flip=True,fill_mode ='nearest' )\n",
    "train_gen = train_generator(df_train,BATCH_SIZE,train_generator_param,target_size=(im_height,im_width))\n",
    "test_gen = train_generator(df_val,BATCH_SIZE, dict(),target_size=(im_height,im_width))\n",
    "\n",
    "train_steps = len(df_train)//BATCH_SIZE\n",
    "valid_steps = len(df_val)//BATCH_SIZE\n",
    "\n",
    "decay_rate = LEARNING_RATE/EPOCHS\n",
    "adam = tf.keras.optimizers.Adam(lr=LEARNING_RATE,beta_1=0.9,beta_2=0.999,epsilon=None,amsgrad=False,weight_decay=decay_rate)\n",
    "# optimizer.build(Adam(lr=LEARNING_RATE,beta_1=0.9,beta_2=0.999,epsilon=None,amsgrad=False,weight_decay=decay_rate))\n",
    "callbacks = [ModelCheckpoint('unetArch.hdf5', verbose=2,save_best_only=True)]\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=dice_coefficient_loss, metrics=[\"binary_accuracy\", iou, dice_coefficients])\n",
    "\n",
    "history =model.fit(train_gen,batch_size=BATCH_SIZE,epochs=EPOCHS,\n",
    "          callbacks = callbacks, validation_data=test_gen, steps_per_epoch=train_steps, \n",
    "          validation_steps=valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_post_training = history.history\n",
    "\n",
    "train_dice_coeff = history_post_training['dice_coefficient']\n",
    "#dice_coefficient & val_dice_coefficient are accessed from history\n",
    "test_dice_coeff = history_post_training['val_dice_coefficient']\n",
    "\n",
    "#Jaccard distance for plotting accuracy\n",
    "train_jaccard_list = history_post_training['iou']\n",
    "test_jaccard_list = history_post_training['val_iou']\n",
    "\n",
    "# Loss for plotting loss\n",
    "train_loss_list = history_post_training['loss']\n",
    "test_loss_list = history_post_training['val_loss']\n",
    "\n",
    "plt.figure(1)\n",
    "# Epochs VS Loss\n",
    "plt.plot(test_loss_list, 'b-')\n",
    "plt.plot(train_loss_list, 'r-')\n",
    "\n",
    "plt.xlabel('iterations')   #epochs\n",
    "plt.ylabel('loss')\n",
    "plt.title('Loss Graph', fontsize =12)\n",
    "\n",
    "plt.figure(2)\n",
    "# Epochs VS Accuracy\n",
    "plt.plot(train_dice_coeff, 'b-')\n",
    "plt.plot(test_dice_coeff, 'r-')\n",
    "\n",
    "plt.xlabel('iterations')   #epochs\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy Graph', fontsize =12)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply trained model in test data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load previously trained model from 'unetArch.hdf5' file\n",
    "model = load_model(unetArch.hdf5, custome_objects = {'dice_coefficient_loss':dice_coefficients_loss,'iou':iou,'dice_coefficient':dice_coefficients})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = train_generator(df_test,BATCH_SIZE, dict(),target_size=(im_height,im_width))\n",
    "\n",
    "results = model.evaluate(test_gen, steps= len(df_test)//BATCH_SIZE)\n",
    "print(\"Test loss: \", results[0])\n",
    "print(\"Test IoU: \", results[1])\n",
    "print(\"Test Dice Coefficient: \", results[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Final Prediction on Test data\"\n",
    "Plotting Predicted Masks Segmentation Results from Test image set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (20):\n",
    "    index = np.random.ranint(1,len(df_test.index))\n",
    "    img = cv2.imread(df_test['image_filenames_train'].iloc[index])\n",
    "    img = cv2.resize(img, (im_height,im_width))\n",
    "    img = img/255   #normalize\n",
    "    #print(img.shape)     (256,256,3) ->3D tensor\n",
    "    #To get 4D array\n",
    "    img = img[np.newaxis, :, :, :]\n",
    "    #print(img.shape)     (1,256,256,3) ->4D array\n",
    "    \n",
    "    predicted_img = model.predict(img)\n",
    "    plt.figure(figsize=(12,12))\n",
    "    \n",
    "    #Plot original image, original mask iamge , predicted image\n",
    "    plt.subplot([1,3,1])   #1row 3coloumn 1st index\n",
    "    plt.imshow(np.squeeze(img))\n",
    "    plt.title('Original image')\n",
    "    \n",
    "    plt.subplot([1,3,2])   #1row 3coloumn 2nd index\n",
    "    plt.imshow(np.squeeze(cv2.imread(df_test['mask'].iloc[index])))\n",
    "    plt.title('Original mask')\n",
    "    \n",
    "    plt.subplot([1,3,3])   #1row 3coloumn 3rd index\n",
    "    plt.imshow(np.squeeze(predicted_img)>0.5)\n",
    "    plt.title('Predicted image')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
